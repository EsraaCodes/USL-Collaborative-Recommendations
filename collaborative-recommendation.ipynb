{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Item based filtering\n",
    "This approach is mostly preferred since the movie don't change much. We can rerun this model once a week unlike User based where we have to frequently run the model.\n",
    "\n",
    "In this kernel, We look at the implementation of Item based filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "movies = pd.read_csv('data/movies.csv') #read csv file in data/movies.csv\n",
    "ratings = pd.read_csv('data/ratings.csv') #read csv file in data/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the head of the ratings to see some of its features\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings dataset has \n",
    "* userId - unique for each user\n",
    "* movieId - using this feature ,we take the title of the movie from movies dataset\n",
    "* rating - Ratings given by each user to all the movies using this we are going to predict the top 10 similar movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the head of the movies to see some of its features\\\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie dataset has \n",
    "* movieId - once the recommendation is done, we get list of all similar movieId and get the title for each movie from this dataset. \n",
    "* genres -  which is not required for this filtering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the ratings from users per movies\n",
    "#in other words, change the form of the table from what you saw in ratings.head()\n",
    "#to something you could see as movie ratings per users\n",
    "#so, transform the dataset of ratings to have the index as movieid.\n",
    "final_dataset = ratings.pivot(index='movieId',columns='userId',values='rating')\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How should you handle missing values here?\n",
    "#for now, just fill the NaN  as 0\n",
    "final_dataset.fillna(0,inplace=True)\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real world, ratings are very sparse and data points are mostly collected from very popular movies and highly engaged users. So we will reduce the noise by adding some filters and qualify the movies for the final dataset.\n",
    "* To qualify a movie, minimum 10 users should have voted a movie.\n",
    "* To qualify a user, minimum 50 movies should have voted by the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the Number of Users Who Voted for Each Movie:\n",
    "# Group the DataFrame by movieId and count the number of ratings for each movie.\n",
    "\n",
    "no_user_voted = ratings.groupby('movieId')['rating'].agg('count')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the Number of Movies Each User Has Voted For:\n",
    "#Group the DataFrame by userId and count the number of ratings each user has made.\n",
    "no_movie_voted=ratings.groupby('userId')['rating'].agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the Number of Users Who Voted for Each Movie:\n",
    "f,ax = plt.subplots(1,1,figsize=(16,4))\n",
    "plt.scatter(no_user_voted.index,no_user_voted, color='mediumseagreen') #scatter plot the number of users voted\n",
    "plt.axhline(y=10,color='r') #add this line as a threshold\n",
    "plt.xlabel('MovieId')\n",
    "plt.ylabel('No. of users voted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now keep the movies that where voted by more than 10 users.\n",
    "final_dataset = final_dataset.loc[no_user_voted[no_user_voted>10].index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,1,figsize=(16,4))\n",
    "plt.scatter(no_movie_voted.index,no_movie_voted,color='mediumseagreen') #scatter plot the number of movies \n",
    "plt.axhline(y=50,color='r') #this is the threshold of movies voted from users that voted more than 50 movies\n",
    "plt.xlabel('UserId')\n",
    "plt.ylabel('No. of votes by user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now keep only the movies that has users voting for minimum of 50 movies\n",
    "final_dataset=final_dataset.loc[:,no_movie_voted[no_movie_voted>50].index]\n",
    "final_dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final_dataset has dimensions of **2121 * 378** where most of the values are sparse. I took only small dataset but for\n",
    "original large dataset of movie lens which has more than **100000** features, this will sure hang our system when this has \n",
    "feed to model. To reduce the sparsity we use csr_matric scipy lib. I'll give an example how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.array([[0,0,3,0,0],[4,0,0,0,2],[0,0,0,0,1]])\n",
    "sparsity = 1.0 - ( np.count_nonzero(sample) / float(sample.size) )\n",
    "print(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_sample = csr_matrix(sample)\n",
    "print(csr_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see there is no sparse value in the csr_sample and values are assigned as rows and column index. for the 0th row and 2nd column , value is 3 . Look at the original dataset where the values at the right place. This is how it works using todense method you can take it back to original dataset.\n",
    "* Most of the sklearn works with sparse matrix. surely this will improve our performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_data = csr_matrix(final_dataset.values)\n",
    "final_dataset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use cosine distance metric which is very fast and preferable than pearson coefficient. Please don't use euclidean distance which will not work when the values are equidistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)\n",
    "knn.fit(csr_data) #fit the sparse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_recommendation(movie_name):\n",
    "    n_movies_to_reccomend = 10\n",
    "    movie_list = movies[movies['title'].str.contains(movie_name)]  \n",
    "    if len(movie_list):        \n",
    "        movie_idx= movie_list.iloc[0]['movieId']\n",
    "        movie_idx = final_dataset[final_dataset['movieId'] == movie_idx].index[0]\n",
    "        \n",
    "        distances , indices = knn.kneighbors(csr_data[movie_idx],n_neighbors=n_movies_to_reccomend+1)    \n",
    "        rec_movie_indices = sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),\\\n",
    "                               key=lambda x: x[1])[:0:-1]\n",
    "        \n",
    "        recommend_frame = []\n",
    "        \n",
    "        for val in rec_movie_indices:\n",
    "            movie_idx = final_dataset.iloc[val[0]]['movieId']\n",
    "            idx = movies[movies['movieId'] == movie_idx].index\n",
    "            recommend_frame.append({'Title':movies.iloc[idx]['title'].values[0],'Distance':val[1]})\n",
    "        df = pd.DataFrame(recommend_frame,index=range(1,n_movies_to_reccomend+1))\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return \"No movies found. Please check your input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_movie_recommendation('Iron Man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_movie_recommendation('Memento')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model works perfectly predicting the recommendation based on user behaviour and past search. So we conclude our \n",
    "collaborative filtering here.\n",
    "#### Now,we will try k-means clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "final_dataset.reset_index(drop=True, inplace=True)\n",
    "X = final_dataset.drop('movieId', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "csr_data = csr_matrix(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "from sklearn.preprocessing import normalize\n",
    "normalized_data=normalize(csr_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why:\n",
    "1.Removes popularity bias <br>\n",
    "2.Makes distance meaningful<br>\n",
    "3.Required for good KMeans results<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply elbow method to find out the best K(number of clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.reset_index(drop=True, inplace=True)\n",
    "clusters=[]\n",
    "for i in range (1,20):\n",
    "    kmeans=KMeans(n_clusters=i,random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    clusters.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the elbow point graph of the k-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1,20),clusters)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this elbow graph, we can see that after n_clusters=5, the graph is getting stabilized. Therefore, the optimal number of clusters for k-means is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with the optimal number of clusters found in the elbow point graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_sample=KMeans(n_clusters=5,init='k-means++',random_state=42)\n",
    "km_sample.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KM_labels=km_sample.labels_\n",
    "KM_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the clusters as a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reduce dimensions to 2D\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=KM_labels, cmap='rainbow', alpha=0.6, s=50)\n",
    "\n",
    "# Plot centroids in PCA space\n",
    "centroids_pca = pca.transform(km_sample.cluster_centers_)\n",
    "plt.scatter(centroids_pca[:,0], centroids_pca[:,1], s=200, c='black', marker='X', label='Centroids')\n",
    "\n",
    "plt.title('KMeans Clustering')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the silhouette score to see if the clusters are good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil_score=silhouette_score(X,KM_labels)\n",
    "sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_movie_recommendation('Iron Man')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
